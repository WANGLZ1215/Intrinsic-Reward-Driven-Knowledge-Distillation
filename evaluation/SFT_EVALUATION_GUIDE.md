# SFT Model GSM8K Evaluation Guide

This document describes how to evaluate SFT models on the GSM8K dataset and what data needs to be saved for comparison with RL training results.

## 1. Evaluation Script Usage

### Basic Usage

```bash
# Evaluate SFT model (using final model)
python scripts/evaluate_sft_model.py \
    --sft_model_path checkpoints/sft_model \
    --config config/training_config.yaml \
    --eval_samples 100 \
    --output_file results/sft_evaluation_results.json \
    --output_jsonl results/sft_evaluation_results.jsonl

# Evaluate specific checkpoint
python scripts/evaluate_sft_model.py \
    --sft_model_path checkpoints/sft_model/checkpoint-5607 \
    --config config/training_config.yaml \
    --eval_samples None \
    --output_file results/sft_checkpoint_5607_results.json \
    --output_jsonl results/sft_checkpoint_5607_results.jsonl

# Run on VAST AI (using relative paths)
python scripts/evaluate_sft_model.py \
    --sft_model_path checkpoints/sft_model \
    --config config/training_config.yaml \
    --eval_samples None \
    --output_file results/sft_evaluation_results.json \
    --output_jsonl results/sft_evaluation_results.jsonl \
    --log_level INFO
```

### Parameter Description

- `--sft_model_path`: SFT model path (required)
  - Can be final model: `checkpoints/sft_model`
  - Or specific checkpoint: `checkpoints/sft_model/checkpoint-5607`
  
- `--config`: Configuration file path (default: `config/training_config.yaml`)

- `--eval_samples`: Number of evaluation samples
  - `None` or omitted: Use full test set (1319 samples)
  - Number: Only evaluate first N samples (for quick testing)

- `--output_file`: Output JSON file path (contains summary statistics)
  - Default: `results/sft_evaluation_results.json`

- `--output_jsonl`: Output JSONL file path (contains detailed results for each sample)
  - Default: `results/sft_evaluation_results.jsonl`
  - JSONL format facilitates line-by-line processing and analysis

- `--log_level`: Log level (default: `INFO`)
  - Options: `DEBUG`, `INFO`, `WARNING`, `ERROR`

- `--log_file`: Log file path (optional)
  - If not specified, automatically generates timestamped log file

## 2. Output Data Format

### 2.1 JSON File (Summary Statistics)

File: `results/sft_evaluation_results.json`

Contains the following data:

```json
{
  "model_type": "SFT",
  "model_path": "checkpoints/sft_model",
  "evaluation_time": "2024-01-01T12:00:00",
  "accuracy": 0.7500,
  "statistics": {
    "total_samples": 1319,
    "correct_samples": 989,
    "incorrect_samples": 330,
    "average_response_length": 245.5,
    "average_answer_extraction_success": 0.95,
    "average_logical_consistency": 0.82,
    "average_answer_correctness_score": 0.75,
    "average_reasoning_steps": 4.2
  },
  "answer_extraction_stats": {
    "successful_extractions": 1253,
    "failed_extractions": 66,
    "success_rate": 0.95
  },
  "individual_results": [
    {
      "index": 0,
      "question": "...",
      "ground_truth": "...",
      "sft_response": "...",
      "is_correct": true,
      ...
    }
  ]
}
```

**Key Metrics:**
- `accuracy`: Overall accuracy (most important)
- `statistics.total_samples`: Total number of samples
- `statistics.correct_samples`: Number of correct samples
- `statistics.average_response_length`: Average response length
- `statistics.average_logical_consistency`: Average logical consistency
- `statistics.average_answer_correctness_score`: Average answer correctness score
- `statistics.average_reasoning_steps`: Average number of reasoning steps

### 2.2 JSONL File (Detailed Sample Results)

File: `results/sft_evaluation_results.jsonl`

Each line is a JSON object containing detailed information for each sample:

```json
{
  "index": 0,
  "question": "Question text",
  "ground_truth": "Ground truth answer (includes reasoning process)",
  "ground_truth_text": "Extracted answer text",
  "ground_truth_num": 42.0,
  "sft_response": "Complete response generated by SFT model",
  "sft_answer_text": "Extracted answer text",
  "sft_answer_num": 42.0,
  "is_correct": true,
  "response_length": 245,
  "reasoning_steps_count": 4,
  "logical_consistency_score": 0.85,
  "answer_extraction_success": true,
  "answer_correctness_score": 1.0,
  "answer_relative_error": 0.0
}
```

**Key Fields:**
- `index`: Sample index (for alignment with RL results)
- `question`: Question text
- `ground_truth`: Ground truth answer (complete)
- `sft_response`: Response generated by SFT model
- `is_correct`: Whether correct (boolean)
- `sft_answer_num`: Extracted numerical answer
- `ground_truth_num`: Ground truth numerical answer
- `logical_consistency_score`: Logical consistency score
- `answer_correctness_score`: Answer correctness score

## 3. Comparison with RL Results

### 3.1 Using Comparison Script

```bash
python scripts/compare_sft_rl_results.py \
    --sft_results results/sft_evaluation_results.json \
    --rl_results results/rl_evaluation_results.json \
    --output results/comparison_report.json
```

### 3.2 Comparison Metrics

The comparison script generates the following comparison metrics:

1. **Accuracy Comparison**
   - SFT accuracy vs RL accuracy
   - Improvement magnitude and percentage

2. **Sample-level Comparison**
   - Number of improved samples (RL correct but SFT incorrect)
   - Number of degraded samples (SFT correct but RL incorrect)
   - Number of unchanged samples

3. **Other Metrics Comparison**
   - Average response length
   - Logical consistency
   - Answer correctness score
   - Number of reasoning steps

### 3.3 Comparison Report Format

```json
{
  "comparison_time": "2024-01-01T12:00:00",
  "sft_model_path": "checkpoints/sft_model",
  "rl_model_path": "checkpoints/rl_model/checkpoint-1000",
  "statistics_comparison": {
    "accuracy": {
      "sft": 0.7500,
      "rl": 0.7800,
      "improvement": 0.0300,
      "improvement_percentage": 4.00
    },
    ...
  },
  "individual_samples_comparison": {
    "total_common_samples": 1319,
    "samples_improved": 45,
    "samples_degraded": 15,
    "samples_unchanged": 1259,
    "improvement_details": [...]
  }
}
```

## 4. Running on VAST AI

### 4.1 Preparation

1. **Upload code and data**
   - Ensure project code is uploaded to VAST AI instance
   - Ensure SFT model files are uploaded (`checkpoints/sft_model/`)

2. **Check paths**
   - Script supports both relative and absolute paths
   - Recommended to use relative paths: `checkpoints/sft_model`

3. **Create output directories**
   ```bash
   mkdir -p results logs
   ```

### 4.2 Run Evaluation

```bash
# Navigate to project directory
cd /path/to/Thesis

# Run evaluation (full test set)
python scripts/evaluate_sft_model.py \
    --sft_model_path checkpoints/sft_model \
    --config config/training_config.yaml \
    --eval_samples None \
    --output_file results/sft_evaluation_results.json \
    --output_jsonl results/sft_evaluation_results.jsonl \
    --log_level INFO

# Wait for evaluation to complete (may take several hours depending on test set size and GPU performance)
```

### 4.3 Check Results

```bash
# Check result files
ls -lh results/sft_evaluation_results.*

# View results summary
python -c "import json; data=json.load(open('results/sft_evaluation_results.json')); print(f'Accuracy: {data[\"accuracy\"]:.4f}'); print(f'Total samples: {data[\"statistics\"][\"total_samples\"]}'); print(f'Correct samples: {data[\"statistics\"][\"correct_samples\"]}')"
```

### 4.4 Download Results

After evaluation completes, download the following files for paper analysis:

1. `results/sft_evaluation_results.json` - Summary statistics
2. `results/sft_evaluation_results.jsonl` - Detailed sample results
3. `logs/evaluate_sft_model_*.log` - Evaluation logs

## 5. Common Issues

### Q1: How long does evaluation take?

A: Depends on test set size and GPU performance:
- Full test set (1319 samples): Approximately 2-4 hours (depends on GPU)
- Can use `--eval_samples 100` to test with small samples first

### Q2: What to do if out of memory?

A:
- Check GPU memory usage
- Reduce `--eval_samples` count
- Use smaller batch size (requires code modification)

### Q3: How to evaluate only part of the samples?

A: Use `--eval_samples` parameter:
```bash
--eval_samples 100  # Only evaluate first 100 samples
```

### Q4: What if result files are too large?

A:
- JSON files are usually small (a few MB)
- JSONL files may be large (tens to hundreds of MB)
- If detailed sample results are not needed, can save only JSON file

### Q5: How to compare with RL results?

A: Use comparison script:
```bash
python scripts/compare_sft_rl_results.py \
    --sft_results results/sft_evaluation_results.json \
    --rl_results results/rl_evaluation_results.json \
    --output results/comparison_report.json
```

## 6. Troubleshooting

If encountering issues, check:
1. Log files: `logs/evaluate_sft_model_*.log`
2. Whether model path is correct
3. Whether configuration file is correct
4. Whether GPU memory is sufficient
