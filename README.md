# Intrinsic-Reward-Driven-Knowledge-Distillation
Official implementation of Intrinsic Reward-Driven Knowledge Distillation (IRKD) for mathematical reasoning in LLMs. The framework reconstructs token-level intrinsic rewards from teacher logits via an inverse soft Bellman operator and optimizes a student model using PPO with LoRA. Evaluated on GSM8K with Qwen2.5 teacherâ€“student setup.
